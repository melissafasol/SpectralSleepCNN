{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f4d40e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "\n",
    "from torch import nn\n",
    "from torchvision import datasets, transforms\n",
    "import torch.optim as optim\n",
    "import math\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "948bbff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SleepDataset(Dataset):\n",
    "    def __init__(self, directory, transform=None):\n",
    "        self.directory = directory\n",
    "        self.transform = transform\n",
    "        self.image_files = [os.path.join(directory, file) for file in os.listdir(directory) if file.endswith('.png')]\n",
    "        self.labels = self._extract_labels()\n",
    "\n",
    "    def _extract_labels(self):\n",
    "        \"\"\" Extract labels from file names and map them to integers.\"\"\"\n",
    "        labels = []\n",
    "        for filename in self.image_files:\n",
    "            parts = filename.split('_')\n",
    "            state = parts[-1].split('.png')[0]  # Split and take the last part before \".png\"\n",
    "            if 'wake' in state:\n",
    "                labels.append(0)\n",
    "            elif 'nrem' in state:\n",
    "                labels.append(1)\n",
    "            elif 'REM' in state:\n",
    "                labels.append(2)\n",
    "            else:\n",
    "                labels.append(None)  # Handle unexpected cases\n",
    "        return labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path = self.image_files[idx]\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        if isinstance(image, torch.Tensor) and isinstance(label, int):\n",
    "            return image, label\n",
    "        else:\n",
    "            print(f\"Error: Invalid types returned from __getitem__: image type {type(image)}, label type {type(label)}\")\n",
    "            return None\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "822a1dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SleepDataLoader:\n",
    "    def __init__(self, dataset, batch_size=32, shuffle=False):\n",
    "        self.dataset = dataset\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "\n",
    "    def __iter__(self):\n",
    "        self.idx = 0\n",
    "        if self.shuffle:\n",
    "            self.indices = torch.randperm(len(self.dataset)).tolist()\n",
    "        else:\n",
    "            self.indices = list(range(len(self.dataset)))\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        if self.idx >= len(self.dataset):\n",
    "            raise StopIteration\n",
    "            \n",
    "        batch_indices = self.indices[self.idx:self.idx+self.batch_size]\n",
    "        batch = [self.dataset[i] for i in batch_indices]\n",
    "        self.idx += self.batch_size\n",
    "        \n",
    "        images, labels = zip(*batch)  # Transpose list of pairs\n",
    "        images = torch.stack(images)  # Stack images into a single tensor\n",
    "        labels = torch.tensor(labels)  # Convert labels list to tensor\n",
    "        return images, labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return math.ceil(len(self.dataset) / self.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "23298ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_path = '/home/melissa/PROJECT_DIRECTORIES/SpectralSleepCNN/data/train/'\n",
    "test_path = '/home/melissa/PROJECT_DIRECTORIES/SpectralSleepCNN/data/validation/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "155f4f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define transforms\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),  # Resize the image to 256x256\n",
    "    transforms.ToTensor(),          # Convert the image to a PyTorch tensor\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalize with ImageNet stats\n",
    "])\n",
    "\n",
    "# Usage\n",
    "train_dataset = SleepDataset(training_path, transform=transform)\n",
    "train_loader = SleepDataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "test_dataset = SleepDataset(test_path, transform=transform)\n",
    "test_loader = SleepDataLoader(test_dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceb7c860",
   "metadata": {},
   "source": [
    "## Check Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1c664e79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 0: Image shape torch.Size([3, 256, 256]), Label 2\n",
      "Sample 1: Image shape torch.Size([3, 256, 256]), Label 2\n",
      "Sample 2: Image shape torch.Size([3, 256, 256]), Label 2\n",
      "Sample 3: Image shape torch.Size([3, 256, 256]), Label 2\n",
      "Sample 4: Image shape torch.Size([3, 256, 256]), Label 1\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    item = train_dataset[i]\n",
    "    if item is None:\n",
    "        print(\"Item is None, error in dataset.\")\n",
    "    else:\n",
    "        image, label = item\n",
    "        print(f\"Sample {i}: Image shape {image.size()}, Label {label}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "402af01b",
   "metadata": {},
   "source": [
    "These dimensions refer to the 1: (3) number of channels in the image, 2 and 3 (256, 256) the height and width of the images "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81482d00",
   "metadata": {},
   "source": [
    "## Check Dataloader Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7ae22f7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 3, 256, 256]) torch.Size([32])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32])\n"
     ]
    }
   ],
   "source": [
    "counter = 0  # Initialize a counter\n",
    "for images, labels in train_loader:\n",
    "    print(images.shape, labels.shape)\n",
    "    counter += 1  # Increment the counter with each iteration\n",
    "    if counter == 10:  \n",
    "        break  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f04f1e21",
   "metadata": {},
   "source": [
    "### Why do you normalise with ImageNet stats?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0379ea24",
   "metadata": {},
   "source": [
    "Many deep learning models available in libraries like PyTorch and TensorFlow are pre-trained on the ImageNet dataset. These models are designed to work best when incoming data is similar to the data they were trained on. By normalizing new input data using the same mean and standard deviation as the ImageNet data, it ensures that the data fed into these models is on a similar scale and distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19ab9ca6",
   "metadata": {},
   "source": [
    "## Building model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c9be301d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2dc90916",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNSleep(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv_stack = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3, stride=1, padding=1), \n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(in_channels=16, out_channels=64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "        )\n",
    "        \n",
    "        # Calculating the size of the output from the last MaxPool2d layer\n",
    "        # Assuming the input images are 256x256, after two pooling layers with kernel_size=2, stride=2:\n",
    "        # Output dimension: (256 / 2) / 2 = 64\n",
    "        # Output size for each feature map is 64x64, and there are 64 feature maps\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc_stack = nn.Sequential(\n",
    "            nn.Linear(64 * 64 * 64, 512),  # Size adjusted to match flattened output\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10),  # Assuming 10 classes for output\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_stack(x)\n",
    "        x = self.flatten(x)\n",
    "        logits = self.fc_stack(x)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b4ff745",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Step [10/143], Loss: 17.9939\n",
      "Epoch [1/5], Step [20/143], Loss: 1.6256\n",
      "Epoch [1/5], Step [30/143], Loss: 0.3285\n",
      "Epoch [1/5], Step [40/143], Loss: 0.0514\n",
      "Epoch [1/5], Step [50/143], Loss: 0.0396\n",
      "Epoch [1/5], Step [60/143], Loss: 0.0614\n"
     ]
    }
   ],
   "source": [
    "sleeptrain = CNNSleep()\n",
    "sleeptrain.to(device)  # Ensure model is on the correct device\n",
    "\n",
    "optimizer = torch.optim.Adam(sleeptrain.parameters(), lr=0.001)  # Define optimizer after the model is instantiated\n",
    "criterion = nn.CrossEntropyLoss()  # Define the loss function\n",
    "\n",
    "num_epochs = 5  # Define the number of epochs for training\n",
    "print_every = 10  # Frequency of printing the loss\n",
    "\n",
    "# Start the training loop\n",
    "for epoch in range(num_epochs):  # Loop over the dataset multiple times\n",
    "    running_loss = 0.0  # Initialize running_loss outside the inner loop\n",
    "    for i, (inputs, labels) in enumerate(train_loader):  # Added enumerate to track iteration count\n",
    "        inputs, labels = inputs.to(device), labels.to(device)  # Move data to the appropriate device\n",
    "\n",
    "        optimizer.zero_grad()  # Zero the parameter gradients\n",
    "\n",
    "        outputs = sleeptrain(inputs)  # Forward pass: compute the predicted outputs by passing inputs to the model\n",
    "        loss = criterion(outputs, labels)  # Compute the loss\n",
    "        loss.backward()  # Backward pass: compute the gradient of the loss w.r.t model parameters\n",
    "        optimizer.step()  # Perform a single optimization step (parameter update)\n",
    "\n",
    "        # Optionally print out loss and other metrics here\n",
    "        running_loss += loss.item()\n",
    "        if (i + 1) % print_every == 0:  # Print every 'print_every' mini-batches\n",
    "            print(f'Epoch [{epoch + 1}/{num_epochs}], Step [{i + 1}/{len(train_loader)}], Loss: {running_loss / print_every:.4f}')\n",
    "            running_loss = 0.0  # Reset the running loss after printing\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4edfd59",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env2023",
   "language": "python",
   "name": "env2023"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
